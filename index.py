from openai import OpenAI
import chromadb
import pandas as pd
import tiktoken
import sqlite3
import os
from datetime import datetime
from dotenv import load_dotenv
from getRequests import atualizar_db_com_wp
from storage import load_posts
import streamlit as st

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("‚ö†Ô∏è A vari√°vel OPENAI_API_KEY n√£o foi encontrada no .env!")

client = OpenAI(api_key=OPENAI_API_KEY)

# Banco vetorial local (persist√™ncia em ./chroma_db)
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection("artigos_demo")

# Tokenizer para dividir textos longos
tokenizer = tiktoken.get_encoding("cl100k_base")


# =====================================
# üß© FUN√á√ïES AUXILIARES
# =====================================

def formatar_data(data_str):
    """Converte datas para o formato YYYY-MM-DD."""
    formatos = ["%Y-%m-%d", "%d/%m/%Y", "%d-%m-%Y"]
    for fmt in formatos:
        try:
            return datetime.strptime(data_str, fmt).strftime("%Y-%m-%d")
        except ValueError:
            pass
    return data_str


def dividir_em_chunks(texto, max_tokens=800):
    """Divide textos longos em blocos menores para embeddings."""
    tokens = tokenizer.encode(texto)
    return [tokenizer.decode(tokens[i:i + max_tokens]) for i in range(0, len(tokens), max_tokens)]


# =====================================
# üì∞ ATUALIZA√á√ÉO AUTOM√ÅTICA DO BANCO
# =====================================
def updatePostsDB():
    print("Atualizando banco de mat√©rias a partir do WordPress...")
    try:
        atualizar_db_com_wp()
    except Exception as e:
        print(f"Erro ao atualizar mat√©rias: {e}")
        
# =====================================
# üíæ POPULAR CHROMA (INDEXA√á√ÉO RAG)
# =====================================
def popular_chroma(df):
    existentes = collection.count()
    novos = 0

    if existentes == 0:
        print("‚öôÔ∏è Banco vetorial vazio. Criando embeddings...")
    else:
        print(f"‚ÑπÔ∏è Banco vetorial j√° cont√©m {existentes} vetores. Verificando novos artigos...")

    for _, row in df.iterrows():
        doc_id = str(row["doc_id"]).strip()
        if not doc_id:
            continue

        # Verifica se o artigo j√° foi indexado no Chroma (pelo doc_id)
        existentes_doc = collection.get(where={"doc_id": doc_id})
        if existentes_doc and existentes_doc.get("ids"):
            continue  # j√° existe

        full_text = (
            f"T√≠tulo: {row['titulo']}\n"
            f"Autor: {row['autor']}\n"
            f"Data: {formatar_data(row['data'])}\n"
            f"Link: {row['link']}\n\n"
            f"{row['conteudo']}"
        )

        # Divide o conte√∫do e gera embeddings
        for j, chunk in enumerate(dividir_em_chunks(full_text)):
            emb = client.embeddings.create(
                model="text-embedding-3-small",
                input=chunk
            ).data[0].embedding

            collection.add(
                ids=[f"{doc_id}-{j}"],
                embeddings=[emb],
                documents=[chunk],
                metadatas=[{
                    "doc_id": doc_id,
                    "titulo": row["titulo"],
                    "data": formatar_data(row["data"]),
                    "autor": row["autor"],
                    "link": row["link"]
                }]
            )

        novos += 1

    print(f"‚úÖ Embeddings atualizados. {novos} novos artigos adicionados ao Chroma.")

df = load_posts()
popular_chroma(df)


# =====================================
# üîç CONSULTA RAG
# =====================================

def getPrompt(caminho="prompt.txt"):
    with open(caminho, "r", encoding="utf-8") as f:
        return f.read()

def consultar_rag(mensagens, top_k=5):
    import streamlit as st

    # ====== 1Ô∏è‚É£ Identifica a √∫ltima pergunta ======
    ultima_pergunta = ""
    for m in reversed(mensagens):
        if m["role"] == "user":
            ultima_pergunta = m["content"]
            break

    if not ultima_pergunta:
        print("‚ö†Ô∏è Nenhuma pergunta do usu√°rio encontrada.")
        return

    # ====== 2Ô∏è‚É£ Verifica se a pergunta faz refer√™ncia √† √∫ltima mat√©ria ======
    referencias = ["essa mat√©ria", "esse artigo", "essa reportagem", "essa an√°lise", "esse texto", "ela", "ele", "isso"]
    referencia_detectada = any(ref in ultima_pergunta.lower() for ref in referencias)

    # ====== 3Ô∏è‚É£ Busca sem√¢ntica (RAG normal) ======
    q_emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=ultima_pergunta
    ).data[0].embedding

    resultados = collection.query(
        query_embeddings=[q_emb],
        n_results=top_k * 3
    )

    docs = resultados["documents"][0]
    metas = resultados["metadatas"][0]

    if not metas:
        print("üß† Nenhum resultado encontrado.")
        return

    artigos_unicos = {}
    links_vistos = set()

    for doc, meta in zip(docs, metas):
        doc_id = str(meta.get("doc_id", "")).strip().lower()
        link = str(meta.get("link", "")).strip().lower()

        if not doc_id or not link:
            continue
        if doc_id in artigos_unicos or link in links_vistos:
            continue

        artigos_unicos[doc_id] = {
            "titulo": meta.get("titulo", "(sem t√≠tulo)").strip(),
            "autor": meta.get("autor", "(sem autor)").strip(),
            "data": meta.get("data", "(sem data)").strip(),
            "link": meta.get("link", "(sem link)").strip(),
            "texto": doc
        }
        links_vistos.add(link)

        if len(artigos_unicos) >= top_k:
            break

    if not artigos_unicos:
        print("üß† Nenhum artigo relevante encontrado.")
        return

    # ====== 4Ô∏è‚É£ Monta o contexto base ======
    contexto = ""
    for artigo in artigos_unicos.values():
        resumo = artigo['texto'].split("\n")
        resumo_texto = "\n".join(resumo[:6])
        contexto += (
            f"T√≠tulo: {artigo['titulo']}\n"
            f"Autor: {artigo['autor']}\n"
            f"Data: {artigo['data']}\n"
            f"Link: {artigo['link']}\n\n"
            f"{resumo_texto}\n---\n"
        )

    # ====== 5Ô∏è‚É£ Se o usu√°rio se referiu √† mat√©ria anterior, adiciona os metadados salvos ======
    if referencia_detectada and "metadados_ultima_materia" in st.session_state:
        meta = st.session_state.metadados_ultima_materia
        contexto = (
            f"(O usu√°rio est√° se referindo √† √∫ltima mat√©ria sugerida anteriormente.)\n"
            f"T√≠tulo: {meta['titulo']}\n"
            f"Autor: {meta['autor']}\n"
            f"Data: {meta['data']}\n"
            f"Link: {meta['link']}\n\n"
            f"{contexto}"
        )

    # ====== 6Ô∏è‚É£ Usa o template do prompt ======
    template_prompt = getPrompt()
    prompt = template_prompt.format(pergunta=ultima_pergunta, contexto=contexto)

    # ====== 7Ô∏è‚É£ Monta o hist√≥rico (√∫ltimas 5 mensagens) ======
    historico = [
        {"role": m["role"], "content": m["content"]}
        for m in mensagens[-5:]
    ]

    # ====== 8Ô∏è‚É£ Monta payload final ======
    mensagens_completas = [
        {"role": "system", "content": "Voc√™ √© NEO, o assistente do portal NeoFeed. Responda de forma clara e com base nos artigos do contexto."},
        *historico,
        {"role": "user", "content": prompt}
    ]

    # ====== 9Ô∏è‚É£ Envia pro modelo ======
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=mensagens_completas,
        temperature=0.2,
        stream=True,
    )

    # ====== üîü Salva metadados da √∫ltima mat√©ria ======
    # (pega a primeira, que normalmente √© a mais relevante)
    primeiro_artigo = list(artigos_unicos.values())[0]
    st.session_state.metadados_ultima_materia = {
        "titulo": primeiro_artigo["titulo"],
        "autor": primeiro_artigo["autor"],
        "data": primeiro_artigo["data"],
        "link": primeiro_artigo["link"]
    }

    return stream
