from core.db.storage import save
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import html
import re
from urllib.parse import urlparse

def limpar_caracteres_agressivo(texto):
    
    if not texto:
        return ""
    
    try:
       
        texto = str(texto)
        texto = html.unescape(texto)
        
        # Remove caracteres de controle (0x00-0x1F, 0x7F-0x9F) exceto tab, newline, return
        texto = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', texto)
        
        # Remove zero-width spaces e outros caracteres invis√≠veis
        texto = texto.replace('\u200b', '').replace('\ufeff', '')
        
        # Corrige caracteres acentuados mal formatados
        correcoes = {
            'aÃÅ': '√°', 'eÃÅ': '√©', 'iÃÅ': '√≠', 'oÃÅ': '√≥', 'uÃÅ': '√∫',
            'aÃÄ': '√†', 'eÃÄ': '√®', 'iÃÄ': '√¨', 'oÃÄ': '√≤', 'uÃÄ': '√π', 
            'aÃÉ': '√£', 'eÃÉ': '·∫Ω', 'iÃÉ': 'ƒ©', 'oÃÉ': '√µ', 'uÃÉ': '≈©',
            'aÃÇ': '√¢', 'eÃÇ': '√™', 'iÃÇ': '√Æ', 'oÃÇ': '√¥', 'uÃÇ': '√ª',
            'cÃß': '√ß', 'nÃÉ': '√±',
            '\\c': '', '\\c‚Äã': '', '\\u0301': '', '\\u0300': ''
        }
        
        for erro, correcao in correcoes.items():
            texto = texto.replace(erro, correcao)
    
        texto = re.sub(r'\s+', ' ', texto)
        texto = texto.encode('utf-8', 'ignore').decode('utf-8')
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na limpeza: {e}")
        texto = re.sub(r'[^\x20-\x7E\u00C0-\u00FF]', '', texto)
    
    return texto.strip()


def atualizar_db_com_wp(url="https://neofeed.com.br/wp-json/wp/v2/posts", page="1"):
    print(f"üì° Buscando posts em: {url}")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "application/json, text/plain, */*",
        "Accept-Charset": "utf-8",
        "Referer": "https://neofeed.com.br/",
    }

    try:
        resp = requests.get(
            url,
            params={
                "per_page": 10,
                "page": page,
                "_fields": "id,date,title,content,link,yoast_head_json"
            },
            headers=headers,
            timeout=30
        )
        
        resp.encoding = 'utf-8'
        
        if resp.status_code == 403:
            print("Erro 403: acesso bloqueado.")
            return
        resp.raise_for_status()
        
    except requests.RequestException as e:
        print(f"Erro na requisi√ß√£o: {e}")
        return

    posts = resp.json()
    if not posts:
        print("Nenhum post encontrado na API.")
        return

    novos = []
    for p in posts:
        post_id = p.get("id")
        if not post_id:
            continue

        link = p.get("link", "").strip()
        if any(proibido in link for proibido in ['/brand-stories/', '/apresentado-por-']):
            continue

        
        data_publicacao = p.get("date", "")[:10]
        conteudo_html = p.get("content", {}).get("rendered", "")

        print(data_publicacao)

        soup = BeautifulSoup(conteudo_html, "html.parser")
        
        for script in soup(["script", "style", "meta", "link"]):
            script.decompose()
            
        conteudo_bruto = soup.get_text(separator=" ", strip=True)
        conteudo_limpo = limpar_caracteres_agressivo(conteudo_bruto)
        
        titulo_html = p.get("title", {}).get("rendered", "")
        titulo_limpo = limpar_caracteres_agressivo(titulo_html)

        parsed = urlparse(link)
        path_parts = parsed.path.strip("/").split("/")
        post_category = path_parts[0]

        categoria = post_category

        autor = p.get("yoast_head_json", {}).get("author", "")
        autor_limpo = limpar_caracteres_agressivo(autor)

        novos.append({
            "doc_id": f"artigo-{post_id}",
            "titulo": titulo_limpo,
            "conteudo": conteudo_limpo,
            "categoria": categoria,
            "autor": autor_limpo,
            "data": data_publicacao,
            "link": link,
        })

    if not novos:
        print("Nenhum post v√°lido encontrado ap√≥s limpeza.")
        return
    try:
        save(novos)
        print(f"{len(novos)} posts SALVOS COM SUCESSO (limpos e validados)!")
            
    except Exception as e:
        print(f"Erro ao salvar no banco: {e}")

        
def fetch_posts(url="https://neofeed.com.br/wp-json/wp/v2/posts", page=1):
    logs = []
    logs.append(f"üì° Buscando posts - P√°gina {page}")

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://neofeed.com.br/",
    }

    try:
        resp = requests.get(
            url,
            params={
                "per_page": 10,
                "page": page,
                "_fields": "id,date,title,content,link,yoast_head_json"
            },
            headers=headers,
            timeout=30
        )

        resp.encoding = "utf-8"

        if resp.status_code == 403:
            return {"status": "error", "logs": ["üö´ Erro 403: acesso bloqueado"]}

        resp.raise_for_status()

    except Exception as e:
        return {"status": "error", "logs": [f"‚ùå Erro na requisi√ß√£o: {e}"]}

    posts = resp.json()
    if not posts:
        return {"status": "empty", "logs": ["‚ÑπÔ∏è Nenhum post encontrado"]}

    novos = []

    for p in posts:
        post_id = p.get("id")
        if not post_id:
            continue

        link = p.get("link", "").strip()
        if any(x in link for x in ["/brand-stories/", "/apresentado-por/"]):
            continue

        data_publicacao = p.get("date", "")[:10]
        conteudo_html = p.get("content", {}).get("rendered", "")

        soup = BeautifulSoup(conteudo_html, "html.parser")
        for tag in soup(["script", "style", "meta", "link"]):
            tag.decompose()

        conteudo = limpar_caracteres_agressivo(
            soup.get_text(separator=" ", strip=True)
        )

        titulo = limpar_caracteres_agressivo(
            p.get("title", {}).get("rendered", "")
        )

        parsed = urlparse(link)
        categoria = parsed.path.strip("/").split("/")[0]

        autor = limpar_caracteres_agressivo(
            p.get("yoast_head_json", {}).get("author", "")
        )

        novos.append({
            "doc_id": f"artigo-{post_id}",
            "titulo": titulo,
            "conteudo": conteudo,
            "categoria": categoria,
            "autor": autor,
            "data": data_publicacao,
            "link": link,
        })

    if not novos:
        return {"status": "empty", "logs": ["‚ÑπÔ∏è Nenhum post v√°lido ap√≥s limpeza"]}

    try:
        save(novos)
        for post in novos:
            logs.append(f"- {post['titulo']}")
        logs.append(f"‚úÖ {len(novos)} posts da p√°gina {page} indexados com sucesso!")
        return {"status": "success", "logs": logs}

    except Exception as e:
        return {"status": "error", "logs": [f"‚ùå Erro ao salvar: {e}"]}


if __name__ == "__main__":
    atualizar_db_com_wp()